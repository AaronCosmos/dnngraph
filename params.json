{"name":"dnngraph","tagline":"A DSL for deep neural networks, supporting Caffe and Torch","body":"DNNGraph - A deep neural network model generation DSL in Haskell\r\n================================================================\r\n\r\nIt consists of several parts:\r\n\r\n-   A DSL for specifying the model. This uses the [lens](http://lens.github.io/) library for elegant, composable constructions, and the *fgl* graph library for specifying the network layout.\r\n-   A set of optimization passes that run over the graph representation to improve the performance of the model. For example, we can take advantage of the fact that several layers types (`ReLU`, `Dropout`) can operate in-place.\r\n-   A set of backends to generate code for the platform. Currently, we generate\r\n    -   Caffe (by generating model `prototxt` files)\r\n    -   Torch (by generating Lua scripts)\r\n-   A set of useful CLI tools for exporting, visualizing and understanding a model (visualization of network structure, parameter density)\r\n\r\nFor a guided example, see a [demonstration IHaskell Notebook](http://bit.ly/17kDYze).\r\n\r\nDSL Examples\r\n------------\r\n\r\nThe following script generates a replica of <https://github.com/BVLC/caffe/blob/master/models/bvlc_alexnet/train_val.prototxt>.\r\n\r\n### AlexNet\r\n\r\n``` haskell\r\n  import           Control.Lens\r\n  import           Control.Monad\r\n\r\n  import           NN.DSL\r\n  import           NN.Examples.ImageNet\r\n  import           NN.Graph\r\n\r\n  alexTrain = train & cropSize' 227 & batchSize' 256 & mirror' True\r\n  alexTest = test & cropSize' 227 & batchSize' 50 & mirror' False\r\n\r\n  alexLrn = lrn & localSize' 5 & alphaLRN' 0.0001 & betaLRN' 0.75\r\n  alexConv = conv & param' alexMult & weightFillerC' (gaussian 0.01) & biasFillerC' zero\r\n  alexIP n = ip n & param' alexMult & weightFillerIP' (gaussian 0.005) & biasFillerIP' (constant 0.1)\r\n  alexPool = maxPool & sizeP' 3\r\n\r\n  alexMult = [def & lrMult' 1 & decayMult' 1, -- weights\r\n              def & lrMult' 2 & decayMult' 0] -- biases\r\n\r\n  -- |Model\r\n  conv1 = alexConv & numOutputC' 96 & kernelSizeC' 11 & strideC' 4\r\n  conv2 = alexConv & numOutputC' 256 & padC' 2 & kernelSizeC' 5 & groupC' 2\r\n  conv3 = alexConv & numOutputC' 384 & padC' 1 & kernelSizeC' 3\r\n  conv4 = alexConv & numOutputC' 384 & padC' 1 & kernelSizeC' 3 & groupC' 2 & biasFillerC' (constant 0.1)\r\n  conv5 = alexConv & numOutputC' 256 & padC' 1 & kernelSizeC' 3 & groupC' 2 & biasFillerC' (constant 0.1)\r\n\r\n  alexNet = do\r\n    -- Set up the model\r\n    (input', representation) <-\r\n        sequential [\r\n             -- Convolutional Layers\r\n             conv1, relu, alexLrn, alexPool & strideP' 3,\r\n             conv2, relu, alexLrn, alexPool & strideP' 2,\r\n             conv3, relu,\r\n             conv4, relu,\r\n             conv5, relu, alexPool & strideP' 2,\r\n             -- FC Layers\r\n             alexIP 4096, relu, dropout 0.5,\r\n             alexIP 4096, relu, dropout 0.5,\r\n             alexIP 1000 & weightFillerIP' (gaussian 0.01) & biasFillerIP' zero]\r\n\r\n    forM_ [alexTrain, alexTest] $ attach (To input')\r\n    forM_ [accuracy 1, accuracy 5, softmax] $ attach (From representation)\r\n```\r\n\r\nor visually, using `NN.Visualize`,\r\n\r\n![](http://i.imgur.com/1hKlPdA.png)\r\n\r\n### GoogLeNet\r\n\r\nThe following script generates a replica of <https://github.com/BVLC/caffe/blob/master/models/bvlc_googlenet/train_val.prototxt>\r\n\r\n``` haskell\r\n  module NN.Examples.GoogLeNet where\r\n\r\n  import           Gen.Caffe.FillerParameter       as FP\r\n  import           Gen.Caffe.InnerProductParameter as IP\r\n  import           Gen.Caffe.LayerParameter        as LP\r\n\r\n  import           Control.Lens\r\n  import           Control.Monad\r\n  import           Data.Sequence                   (singleton)\r\n  import           Data.Word\r\n\r\n  import           NN\r\n  import           NN.Examples.ImageNet\r\n\r\n\r\n  googleTrain = train & mirror' True & batchSize' 32 & cropSize' 224\r\n  googleTest = test & mirror' False & batchSize' 50 & cropSize' 224\r\n\r\n  googleMult = [def & lrMult' 1 & decayMult' 1, -- weights\r\n                def & lrMult' 2 & decayMult' 0] -- biases\r\n  googleConv = conv & param' googleMult & biasFillerC' (constant 0.2)\r\n  googleLRN = lrn & localSize' 5 & alphaLRN' 0.0001 & betaLRN' 0.75\r\n  googlePool = maxPool & sizeP' 3 & strideP' 2\r\n  googleIP n = ip n & param' googleMult\r\n\r\n  conv1 = googleConv & numOutputC' 64 & padC' 3 & kernelSizeC' 7 & strideC' 2 & weightFillerC' (xavier 0.1)\r\n  conv2 = googleConv & numOutputC' 192 & padC' 1 & kernelSizeC' 3 & weightFillerC' (xavier 0.03)\r\n\r\n  topPool = avgPool & sizeP' 7 & strideP' 1\r\n  topFc = googleIP 1000 & biasFillerIP' (constant 0) & weightFillerIP' (xavier 0.0)\r\n          -- Weird, but in Caffe replication\r\n          & _inner_product_param._Just.IP._weight_filler._Just._std .~ Nothing\r\n\r\n  data Inception = Inception {_1x1, _3x3reduce, _3x3, _5x5reduce, _5x5, _poolProj :: Word32}\r\n\r\n  inception :: Node -> Inception -> NetBuilder Node\r\n  inception input Inception{..} = do\r\n    columns' <- mapM sequential columns\r\n    concat'' <- layer' concat'\r\n    forM_ columns' $ \\(bottom, top) -> do\r\n                                    input >-> bottom\r\n                                    top >-> concat''\r\n    return concat''\r\n      where\r\n        columns = [\r\n         [googleConv & numOutputC' _1x1  & kernelSizeC' 1 & weightFillerC' (xavier 0.03), relu],\r\n         [googleConv & numOutputC' _3x3reduce & kernelSizeC' 1 & weightFillerC' (xavier 0.09), relu, googleConv & numOutputC' _3x3 & kernelSizeC' 3 & weightFillerC' (xavier 0.03) & padC' 1, relu],\r\n         [googleConv & numOutputC' _5x5reduce & kernelSizeC' 1 & weightFillerC' (xavier 0.2), relu, googleConv & numOutputC' _5x5 & kernelSizeC' 5 & weightFillerC' (xavier 0.03) & padC' 2, relu],\r\n         [maxPool& sizeP' 3 & strideP' 3 & padP' 1, googleConv & numOutputC' _poolProj & kernelSizeC' 1 & weightFillerC' (xavier 0.1), relu]]\r\n\r\n  intermediateClassifier :: Node -> NetBuilder ()\r\n  intermediateClassifier source = do\r\n    (input, representation) <- sequential [pool1, conv1', relu, fc1, relu, dropout 0.7, fc2]\r\n    source >-> input\r\n\r\n    forM_ [accuracy 1, accuracy 5, softmax & _loss_weight <>~ singleton 0.3] $ attach (From representation)\r\n      where\r\n        pool1 = avgPool & sizeP' 5 & strideP' 3\r\n        conv1' = googleConv & numOutputC' 128 & kernelSizeC' 1 & weightFillerC' (xavier 0.08)\r\n        fc1 = googleIP 1024 & weightFillerIP' (xavier 0.02) & biasFillerIP' (constant 0.2)\r\n        fc2 = googleIP 1000 & weightFillerIP' (xavier 0.0009765625) & biasFillerIP' (constant 0)\r\n\r\n  -- What to do at each row in the inner column?\r\n  data Row = I Inception | Classifier | MaxPool\r\n\r\n  insertRow :: Node -> Row -> NetBuilder Node\r\n  insertRow input (I inceptor) = inception input inceptor\r\n  insertRow input Classifier = do\r\n    intermediateClassifier input\r\n    return input\r\n  insertRow input MaxPool = do\r\n    node <- layer' googlePool\r\n    input >-> node\r\n    return node\r\n\r\n  googLeNet :: NetBuilder ()\r\n  googLeNet = do\r\n    (input, initial) <- sequential [conv1, relu, googlePool, googleLRN, conv2, relu, googleLRN, googlePool]\r\n\r\n    top <- foldM insertRow initial [\r\n               I $ Inception 64 96 128 16 32 32,\r\n               I $ Inception 128 128 192 32 96 64,\r\n               MaxPool,\r\n               I $ Inception 192 96 208 16 48 64,\r\n               Classifier,\r\n               I $ Inception 150 112 224 24 64 64,\r\n               I $ Inception 128 128 256 24 64 64,\r\n               I $ Inception 112 144 288 32 64 64,\r\n               Classifier,\r\n               I $ Inception 256 160 320 32 128 128,\r\n               MaxPool,\r\n               I $ Inception 256 160 320 32 128 128,\r\n               I $ Inception 384 192 384 48 128 128]\r\n\r\n    (_, representation) <- with top >- sequential [topPool, dropout 0.4, topFc]\r\n\r\n    forM_ [accuracy 1, accuracy 5, softmax] $ attach (From representation)\r\n    forM_ [googleTrain, googleTest] $ attach (To input)\r\n\r\n  main :: IO ()\r\n  main = cli googLeNet\r\n```\r\n\r\nCLI Usage\r\n---------\r\n\r\nIn the GoogLeNet example, above, we included the line `main ` cli googLeNet=. This generates a CLI for our model that can be accessed with `runhaskell /path/to/our/model.hs`. Currently, we can\r\n\r\n-   export to Caffe\r\n-   export to Torch\r\n-   visualize the network structure.\r\n\r\nFor example:\r\n\r\n    $ runhaskell NN/Examples/GoogLeNet.hs --help\r\n    Usage: GoogLeNet.hs COMMAND\r\n\r\n    Available options:\r\n      -h,--help                Show this help text\r\n\r\n    Available commands:\r\n      caffe                    Generate a Caffe .prototxt to run with `caffe train\r\n                               --model=<>\r\n      torch                    Generate Lua code to be `require`'d into an existing\r\n                               Torch script\r\n      pdf                      Generate a PDF visualizing the model's connectivity\r\n\r\n    $ runhaskell NN/Examples/GoogLeNet.hs caffe --output /tmp/x.prototxt\r\n    $ runhaskell NN/Examples/GoogLeNet.hs pdf --output /tmp/x.pdf\r\n\r\nCaffe Backend\r\n-------------\r\n\r\nThe Caffe backend generates a Caffe `.prototxt` that can be run with `caffe train --model=<>`, without any modification necessary.\r\n\r\nTorch Backend\r\n-------------\r\n\r\nThe Torch backend generates Lua code that can be imported directly into an existing Torch script.\r\n\r\nAnything network that can be expressed as a nested combination of computational layers, combined with `nn.Sequential`, `nn.Concat`, `nn.ModelParallel`, `nn.DataParallel` etc can be generated under this framework.\r\n\r\nFor an example output, the model specified as\r\n\r\n``` haskell\r\n  alexTrain = train & cropSize' 227 & batchSize' 256 & mirror' True\r\n  alexTest = test & cropSize' 227 & batchSize' 50 & mirror' False\r\n\r\n  alexConv = conv & param' alexMult & weightFillerC' (gaussian 0.01) & biasFillerC' zero\r\n  alexPool = maxPool & sizeP' 3\r\n\r\n  conv1 = alexConv & numOutputC' 96 & kernelSizeC' 11 & strideC' 4\r\n  pool1 = alexPool & strideP' 3\r\n\r\n  model = do\r\n    (input', representation) <- sequential [conv1, relu, pool1]\r\n    forM_ [alexTrain, alexTest] $ attach (To input')\r\n    forM_ [accuracy 1, accuracy 5, softmax] $ attach (From representation)\r\n```\r\n\r\ngenerates the following code:\r\n\r\n``` lua\r\n  require(\"nn\")\r\n  require(\"cunn\")\r\n  local seq0 = nn.Sequential()\r\n  seq0:add(nn.SpatialConvolutionMM(nil, 96, 11, 11, 4, 4, 0))\r\n  seq0:add(nn.Threshold())\r\n  seq0:add(nn.SpatialMaxPooling(3, 3, 3, 3))\r\n  seq0:add(nn.LogSoftMax())\r\n  local criterion1 = nn.ClassNLLCriterion()\r\n  return seq0, criterion1\r\n```\r\n\r\nFor a more complicated example, the network specified as\r\n\r\n``` haskell\r\n  do\r\n    x <- layer' relu\r\n    (_, y) <- with x >- sequential [conv, relu, maxPool, conv, relu]\r\n    (_, z) <- with x >- sequential [conv, relu, maxPool, conv, relu]\r\n    concat'' <- layer' concat'\r\n\r\n    y >-> concat''\r\n    z >-> concat''\r\n    _ <- with concat'' >- sequential [ip 4096, relu, dropout 0.5, ip 1000, softmax]\r\n    return ()\r\n```\r\n\r\nthat looks like\r\n\r\n[![](http://i.imgur.com/dsqgYna.png)](http://i.imgur.com/dsqgYna.png)\r\n\r\nwill generate\r\n\r\n``` lua\r\nrequire(\"nn\")\r\nlocal seq0 = nn.Sequential()\r\nlocal mod1 = nn.Threshold()\r\nseq0:add(mod1)\r\nlocal concat2 = nn.DepthConcat()\r\nlocal seq3 = nn.Sequential()\r\nlocal mod4 = nn.SpatialConvolutionMM(nil, nil, nil, nil, 1, 1, 0)\r\nseq3:add(mod4)\r\nlocal mod5 = nn.Threshold()\r\nseq3:add(mod5)\r\nlocal mod6 = nn.SpatialMaxPooling(nil, nil, 1, 1)\r\nseq3:add(mod6)\r\nlocal mod7 = nn.SpatialConvolutionMM(nil, nil, nil, nil, 1, 1, 0)\r\nseq3:add(mod7)\r\nlocal mod8 = nn.Threshold()\r\nseq3:add(mod8)\r\nconcat2:add(seq3)\r\nlocal seq9 = nn.Sequential()\r\nlocal mod10 = nn.SpatialConvolutionMM(nil, nil, nil, nil, 1, 1, 0)\r\nseq9:add(mod10)\r\nlocal mod11 = nn.Threshold()\r\nseq9:add(mod11)\r\nlocal mod12 = nn.SpatialMaxPooling(nil, nil, 1, 1)\r\nseq9:add(mod12)\r\nlocal mod13 = nn.SpatialConvolutionMM(nil, nil, nil, nil, 1, 1, 0)\r\nseq9:add(mod13)\r\nlocal mod14 = nn.Threshold()\r\nseq9:add(mod14)\r\nconcat2:add(seq9)\r\nseq0:add(concat2)\r\nlocal mod15 = nn.Linear(nil, 4096)\r\nseq0:add(mod15)\r\nlocal mod16 = nn.Threshold()\r\nseq0:add(mod16)\r\nlocal mod17 = nn.Dropout(0.5)\r\nseq0:add(mod17)\r\nlocal mod18 = nn.Linear(nil, 1000)\r\nseq0:add(mod18)\r\nlocal mod19 = nn.LogSoftMax()\r\nseq0:add(mod19)\r\nlocal criteria20 = nn.ClassNLLCriterion()\r\nreturn seq0, criteria20\r\n```\r\n\r\nVisualization Examples\r\n----------------------\r\n\r\nThe `NN.Visualize` module provides some plotting tools. To use these,\r\n\r\n``` haskell\r\n  import NN.Visualize\r\n\r\n  visualize :: Net -> DotGraph Node\r\n  png :: FilePath -> DotGraph Node -> IO FilePath\r\n\r\n  -- For example, to visualize GoogLeNet to a file\r\n  file :: FilePath\r\n  (frontend googLeNet & visualize & png file) :: IO FilePath\r\n```\r\n\r\nAn example output is (click for higher resolution):\r\n\r\n![](http://i.imgur.com/ScvjNmT.jpg)\r\n\r\nParameter Sweeps\r\n----------------\r\n\r\nTo use this, write your model generation script as a Haskell file, and then (for example)\r\n\r\n``` bash\r\n  caffe train --model <(runhaskell Model.hs) --solver=solver.prototxt\r\n```\r\n\r\nTo perform a parameter sweep, use the parameterizing\r\n\r\n``` bash\r\n  for model in $(runhaskell Model.hs); do\r\n      caffe train --model=$model --solver=solver.prototxt\r\n  done\r\n```\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}